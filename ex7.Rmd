---
title: "21bcs022-ex6"
author: "Nagajothi.R"
date: "`r Sys.Date()`"
output:
  word_document: default
  html_document: default
  pdf_document: default
---

```{r }

#Common Question:
library(tidyverse)
library(modelr)
#simple model
ggplot(sim1, aes(x, y)) +
				geom_point()
models <- tibble(
a1 = runif(250, -20, 40),
a2 = runif(250, -5, 5)
)
ggplot(sim1, aes(x, y)) +
geom_abline(
aes(intercept = a1, slope = a2),
data = models, alpha = 1/4
) + geom_point()
model1 <- function(a, data) 
{ 
		a[1] + data$x * a[2] 
}
model1(c(7, 1.5), sim1)

#Root Mean Squared Derivation:
measure_distance <- function(mod, data) 
{
	 diff <- data$y - model1(mod, data) 
    sqrt(mean(diff ^ 2)) 
}   
measure_distance(c(7, 1.5), sim1)
sim1_dist <- function(a1, a2) 
{ 
	measure_distance(c(a1, a2), sim1) 
} 
models <- models %>% 
mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist)) 
models 
ggplot(sim1, aes(x, y)) + 
	geom_point(size = 2, color = "grey30") + 
		geom_abline( 
			aes(intercept = a1, slope = a2, color = -dist),
			data = filter(models, rank(dist) <= 10) )
ggplot(models, aes(a1, a2)) + 
	geom_point( 
		data = filter(models, rank(dist) <= 10), 
		size = 4, color = "red" ) + 
geom_point(aes(color = -dist))

#Grid Search:
grid <- expand.grid( 
		a1 = seq(-5, 20, length = 25), 
		a2 = seq(1, 3, length = 25)
	 ) %>% 
mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))

grid %>% 
		ggplot(aes(a1, a2)) + 
		geom_point( 
			data = filter(grid, rank(dist) <= 10), 
			size = 4, colour = "red" ) + 
		geom_point(aes(color = -dist))
ggplot(sim1, aes(x, y)) + 
		geom_point(size = 2, color = "grey30") + 
		geom_abline( 
		         aes(intercept = a1, slope = a2, color = -dist), 
			data = filter(grid, rank(dist) <= 10) )

#Newtonâ€“Raphson search
best <- optim(c(0, 0), measure_distance, data = sim1) 
best$par	#best set of parameters found
#> [1] 4.22 2.05
	ggplot(sim1, aes(x, y)) + 
		geom_point(size = 2, color = "grey30")+
	  geom_abline(intercept = best$par[1], slope =best$par[2])
sim1_mod <- lm(y ~ x, data = sim1) 
coef(sim1_mod)


#Batch Questions:
#1.Create the synthetic data.
#exam score(out of 100) vs time (hours) for 120 students 
#by taking binomial distributed values for time [1:12].
#and compute exam score using score = time * 7 +R; 
#where R is the random value in [-7:16]
#2.Use simple linear regression and find the model.
#3.Do performance evaluation for the model.
#4.Visualize the actual vs. predicted values.

set.seed(0)
n_students <- 120
x<- rbinom(n_students,size=12,prob=0.5)
R_values <- sample(-7:16, n_students, replace = TRUE)
y<- sapply(1:n_students, function(i) {
 time <- x[i]
  R <- R_values[i]
  score = time * 7 +R; 
  return(score)
})
data=data.frame(x,y)
print(head(data))

train_index <- sample(1:length(x), 0.7 * length(x))
x_train <- x[train_index]
y_train <- y[train_index]
x_test <- x[-train_index]
y_test <- y[-train_index]
df_train <- data.frame(x = x_train, y = y_train)
df_test <- data.frame(x = x_test, y = y_test)

lm_model <- function(df_train) {
  mean_x <- mean(df_train$x)
  mean_y <- mean(df_train$y)
  beta1 <- sum((df_train$x - mean_x) * (df_train$y - mean_y)) / 
    sum((df_train$x - mean_x)^2)
  beta0 <- mean_y - beta1 * mean_x
  
  # Return the coefficients
  return(c(beta0 = beta0, beta1 = beta1))
}

model <- lm_model(df_train)

l_predict <- function(model, x_test) {
  y <- (model[2] * x_test) + model[1]
  return(y)
}

y_predict <- l_predict(model, x_test)

# Calculate Evaluation metrics
mse <- mean((df_test$y - y_predict)^2)
rmse <- sqrt(mse)

# Print evaluation metrics
cat("Mean Squared Error (MSE):", mse, "\n")
cat("Root Mean Squared Error (RMSE):", rmse, "\n")

# Plot actual vs. predicted values
plot(x_test, y_test, col = "blue", pch = 16, 
     xlab = "x", ylab = "y", main = "Actual vs Predicted")
points(x_test, y_predict, col = "red", pch = 16)
legend("topleft", legend = c("Actual", "Predicted"), 
       col = c("blue", "red"), pch = 16)






```
